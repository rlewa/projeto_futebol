{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlewa/projetofutebol/blob/main/notebooks/data/Scraping6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dju4bTancZ5",
        "outputId": "64c17679-55c0-401a-97c1-5601569a3faa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPePkSUl-YA1"
      },
      "source": [
        "!pip install git+'https://github.com/rlewa/projetofutebol'\n",
        "from packages.packages import *\n",
        "from packages.functions import *\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mPiohXMtZMP"
      },
      "source": [
        "Scraping news from the website: https://www.thesun.co.uk/sport/football/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J2S_QIJtXQS"
      },
      "source": [
        "def get_teams_links_thesun(URL: str = 'https://www.thesun.co.uk/sport/football/premierleague/') -> dict:\n",
        "\n",
        "  '''\n",
        "    Gets the links for each team available on 'thesun.co.uk' website\n",
        "\n",
        "    Input:\n",
        "      str: website link\n",
        "      bool: pl_teams that is True for only Premier League teams and False for any team \n",
        "\n",
        "    Return:\n",
        "      dict: dict with all teams names and links available\n",
        "  '''\n",
        "\n",
        "  req = requests.get(URL)\n",
        "  soup = BeautifulSoup(req.text, 'html.parser')\n",
        "\n",
        "  teams_links = np.array([])\n",
        "\n",
        "  for link in soup.find_all('a', class_ = \"theme__before-color\"):\n",
        "    teams_links = np.append(teams_links, str(link).split('<a class=\"theme__before-color\" href=\"')[1].split('\"></a>')[0])\n",
        "  \n",
        "  teams_links = teams_links[:20]\n",
        "\n",
        "  return teams_links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klU7A43OvfaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee51029-f1ab-4655-dc9d-ac651cb47bff"
      },
      "source": [
        "team_links = get_teams_links_thesun()\n",
        "team_links"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['https://www.thesun.co.uk/sport/football/team/1196653/arsenal/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196670/aston-villa/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1225644/brentford/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/3571589/brighton-hove-albion/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1225466/burnley/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196660/chelsea/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196666/crystal-palace/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196662/everton/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1225859/leeds-united/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196651/leicester-city/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196659/liverpool/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196654/manchester-city/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196656/manchester-united/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196669/newcastle-united/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196667/norwich-city/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196657/southampton/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196652/tottenham-hotspur/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196664/watford/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1196655/west-ham/',\n",
              "       'https://www.thesun.co.uk/sport/football/team/1226009/wolverhampton-wanderers/'],\n",
              "      dtype='<U77')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlMjlwRvrki"
      },
      "source": [
        "def get_news_thesun(name):\n",
        "\n",
        "  '''\n",
        "    Gets all the news from the thesun.co.uk website: 'date', 'hour', title' and 'text'\n",
        "\n",
        "    Input:\n",
        "    Returns:\n",
        "      bool: True if works\n",
        "  '''\n",
        "\n",
        "  standard_names = ['Arsenal FC', 'Aston Villa', 'Brentford FC', 'Brighton & Hove Albion', 'Burnley FC', 'Chelsea FC', 'Crystal Palace', \n",
        "                    'Everton FC', 'Leeds United', 'Leicester City', 'Liverpool FC', 'Manchester City', 'Manchester United', 'Newcastle United', \n",
        "                    'Norwich City', 'Southampton FC', 'Tottenham Hotspur', 'Watford FC', 'West Ham United', 'Wolverhampton Wanderers']\n",
        "\n",
        "  teams_links = get_teams_links_thesun()\n",
        "  \n",
        "  if name == 'gabriel':\n",
        "    teams_links = teams_links[:5]\n",
        "    standard_names = ['Arsenal FC', 'Aston Villa', 'Brentford FC', 'Brighton & Hove Albion', 'Burnley FC']\n",
        "  \n",
        "  elif name == 'pet':\n",
        "    teams_links = teams_links[5:7]\n",
        "    standard_names = ['Chelsea FC', 'Crystal Palace']\n",
        "  \n",
        "  else:\n",
        "    teams_links = teams_links[12:14]\n",
        "    standard_names = ['Manchester United', 'Newcastle United']\n",
        "\n",
        "  index = -1\n",
        "  \n",
        "  for team_link in teams_links:\n",
        "\n",
        "    # index to change teams names\n",
        "    index += 1\n",
        "\n",
        "    print('\\nTeam Name: ', standard_names[index])\n",
        "\n",
        "    dict_news = {'date': [], 'hour': [], 'title': [], 'text': []}\n",
        "\n",
        "    # Enter the first iteration of while.\n",
        "    news_available = True \n",
        "    \n",
        "    count_page = 1\n",
        "\n",
        "    while news_available:\n",
        "      try:\n",
        "\n",
        "          string = team_link + \"page/\" + str(count_page) + \"/\"\n",
        "\n",
        "          req  = requests.get(string) \n",
        "\n",
        "          if req.status_code == 200:\n",
        "            \n",
        "            soup = BeautifulSoup(req.text, 'html.parser')\n",
        "            \n",
        "            news_links = []\n",
        "            \n",
        "            # for each link, we get everything related to that news: 'date', 'title' and 'text'\n",
        "\n",
        "            # find all 'a' in the class 'teaser-anchor', it represents all news links\n",
        "            for a in soup.find_all('a', class_=\"teaser-anchor\", href = True):\n",
        "              link_text = str(a).split('href=\"')[1].split('\"></a>')[0]\n",
        "              \n",
        "              if 'uncategorized' not in link_text:\n",
        "                news_links.append(link_text)\n",
        "\n",
        "            for link in news_links:\n",
        "              \n",
        "              #print(link)\n",
        "\n",
        "              req = requests.get(link)\n",
        "              soup_link = BeautifulSoup(req.text, 'html.parser')\n",
        "              \n",
        "              # only get news, not live blogs\n",
        "              if soup_link.find(class_ = \"text-slug t-p-background-color text-slug__live-blog\") is None:\n",
        "                \n",
        "                # get main text \n",
        "                full_text = ' '\n",
        "                for text in soup_link.find_all('div', class_=\"article__content\"):\n",
        "                  soup2 = BeautifulSoup(str(text), 'html.parser')\n",
        "                  for p in soup2.find_all('p'):\n",
        "                    if '<p class=\"has-text-align-center\">' not in str(p) and '<strong>' not in str(p) and '<p>[video' not in str(p) and '<p dir=\"ltr\"' not in str(p) and '<a href=\"https://twitter.com/goonerdave66\"' not in str(p):\n",
        "                      text = p.get_text()\n",
        "                      text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
        "                      text = re.sub(r'pic.twitter.com/[\\w]*',\"\", text)\n",
        "                      full_text += ' ' + text + ' '\n",
        "\n",
        "                full_text = full_text.strip()\n",
        "                if len(full_text) == 0:\n",
        "                  continue\n",
        "\n",
        "                dict_news['text'].append(full_text)\n",
        "\n",
        "                date_ = soup_link.find('span', class_ = \"article__timestamp\")\n",
        "\n",
        "                # specific html tag when it gets the time instead of date\n",
        "                if \":\" in str(date_):\n",
        "                  news_date = soup_link.find('div', class_ = \"article__published\")\n",
        "                  news_hour = soup_link.find('span', class_ = \"article__timestamp\")\n",
        "                  news_title = soup_link.find('section', class_ = \"article__headline-section\")\n",
        "                  \n",
        "                  if news_date is None:\n",
        "                    dict_news['date'].append('')\n",
        "                  dict_news['date'].append(news_date.get_text().replace(\",\", \" \").strip()) \n",
        "\n",
        "                  if news_hour is None:\n",
        "                    dict_news['hour'].append('')\n",
        "                  dict_news['hour'].append(news_hour.get_text().strip() )\n",
        "\n",
        "                  if news_title is None:\n",
        "                    dict_news['title'].append('')\n",
        "                  dict_news['title'].append(news_title.h1.text.replace(\"\\n\", ' ').replace(\"\\t\", ' ').strip())\n",
        "\n",
        "                else:\n",
        "                  news_title = soup_link.find('section', class_ = \"article__headline-section\")\n",
        "                  news_hour = soup_link.find('span', class_ = \"article__datestamp\")\n",
        "                  news_date = soup_link.find('span', class_ = \"article__timestamp\")\n",
        "\n",
        "                  if news_date is None:\n",
        "                    dict_news['date'].append('')\n",
        "                  dict_news['date'].append(news_date.text.replace(',', '').strip()) \n",
        "\n",
        "                  if news_hour is None:\n",
        "                    dict_news['hour'].append('')\n",
        "                  dict_news['hour'].append(news_hour.text.replace(',', '').strip())\n",
        "\n",
        "                  if news_title is None:\n",
        "                    dict_news['title'].append('')\n",
        "                  dict_news['title'].append(news_title.h1.text.replace(\"\\n\", ' ').replace(\"\\t\", ' ').strip())\n",
        "                  \n",
        "              else:\n",
        "                continue\n",
        "\n",
        "      except Exception as erro:\n",
        "\n",
        "          if str(erro).lower() == 'list index out of range':\n",
        "            break\n",
        "\n",
        "          else:\n",
        "            pass\n",
        "\n",
        "      finally:\n",
        "        \n",
        "        if news_links == []:\n",
        "          break\n",
        "\n",
        "        if count_page % 5 == 0:\n",
        "          print('Page number:', count_page, 'is OK!')        \n",
        "        \n",
        "        #Creates dataframe before takes the next team.\n",
        "        df = pd.DataFrame(dict_news, columns=['date', 'hour', 'title', 'text'])  \n",
        "        df['date'] = df['date'].apply(lambda x: x.split(' ')[0].replace('nd', '').replace('rd', '').replace('th', '').replace('st', '') + ' ' +\n",
        "                                      x.split(' ')[1] + ' ' + x.split(' ')[2])\n",
        "\n",
        "        df['date'] = df['date'].apply(lambda x: date.isoformat(dateutil.parser.parse(x).date())) \n",
        "\n",
        "        df.drop_duplicates(inplace = True)\n",
        "    \n",
        "        # Create path and save as csv.\n",
        "\n",
        "        PATH = f'/content/drive/MyDrive/GitHub/Dados/Noticias/Scraping5/{standard_names[index]}.csv'\n",
        "        df.to_csv(PATH, index = False)\n",
        "\n",
        "        count_page += 1\n",
        "        pass\n",
        "      \n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_news_thesun('gabriel')"
      ],
      "metadata": {
        "id": "TR2Qx7jtDYtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7826tCjGErI"
      },
      "source": [
        "#get_news_thesun('pet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_news_thesun('kaue')"
      ],
      "metadata": {
        "id": "7zkqeJF8pF4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4h9i46VIpJOj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}